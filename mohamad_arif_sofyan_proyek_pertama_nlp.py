# -*- coding: utf-8 -*-
"""Mohamad Arif Sofyan_Proyek Pertama_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10dbJDvWgnKScknt3NreFF0vMubUF8jZa
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import pustaka yang diperlukan"""

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from keras.initializers import glorot_normal

# Download data untuk pemrosesan
nltk.download('stopwords')
nltk.download('punkt')

"""# Import Data"""

df = pd.read_csv("/content/drive/MyDrive/Dicoding/PENGEMBANGAN MACHINE LEARNING/PROYEK 1 - NLP/twitter_training.csv")
df.columns = ['id', 'information', 'label', 'text']
df.head()

df.info()

df.nunique()

# Menghapus kolom 'id' dan 'information'
df.drop(['id', 'information'], axis=1, inplace=True)
df.head()

# Cek isi kolom 'label'
df.label.value_counts()

# Cek missing value
df.isna().sum()

# Menghapus baris yang mengandung missing value
df.dropna(inplace=True)

# Menampilkan informasi tentang missing value setelah penghapusan
print("\nJumlah missing value setelah penghapusan:")
print(df.isnull().sum())

# Cek Duplicate
df.duplicated().sum()

# Menghapus baris yang merupakan duplikat
df.drop_duplicates(inplace=True)

# Menampilkan jumlah duplikat setelah penghapusan
print("Jumlah duplikat setelah penghapusan:", df.duplicated().sum())

# Reset indeks setelah menghapus nilai yang hilang dan duplikat
df.reset_index(drop=True, inplace=True)
df.head()

df.shape

"""# Exploratory Data Analysis

Membuat Plot Representasi Label
"""

# Menghitung jumlah data pada setiap kategori label
label_counts = df['label'].value_counts()

# Membuat plot bar untuk representasi label
plt.figure(figsize=(8, 6))
label_counts.plot(kind='bar', color=['red', 'green', 'blue', 'gray'])
plt.title('Representasi Label')
plt.xlabel('Label')
plt.ylabel('Jumlah Data')
plt.show()

"""Membuat plot kata yang sering muncul untuk setiap kategori label"""

# Mengelompokkan teks berdasarkan label
negative_text = ' '.join(df[df['label'] == 'Negative']['text'])
positive_text = ' '.join(df[df['label'] == 'Positive']['text'])
neutral_text = ' '.join(df[df['label'] == 'Neutral']['text'])
irrelevant_text = ' '.join(df[df['label'] == 'Irrelevant']['text'])

# Membuat WordCloud untuk setiap label
wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
wordcloud_neutral = WordCloud(width=800, height=400, background_color='white').generate(neutral_text)
wordcloud_irrelevant = WordCloud(width=800, height=400, background_color='white').generate(irrelevant_text)

# Menampilkan WordCloud
plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.title('WordCloud for Negative')
plt.axis('off')

plt.subplot(2, 2, 2)
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.title('WordCloud for Positive')
plt.axis('off')

plt.subplot(2, 2, 3)
plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.title('WordCloud for Neutral')
plt.axis('off')

plt.subplot(2, 2, 4)
plt.imshow(wordcloud_irrelevant, interpolation='bilinear')
plt.title('WordCloud for Irrelevant')
plt.axis('off')

plt.tight_layout()
plt.show()

"""# Membuat fungsi untuk membersihkan teks"""

def clean_text(text):
    # Konversi teks ke lowercase
    text = text.lower()
    # Menghilangkan tanda baca dan mengubah teks menjadi kata-kata
    tokenizer = RegexpTokenizer(r"\w+")
    tokens = tokenizer.tokenize(text)

    # Menghilangkan stopwords
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in tokens if word.lower() not in stop_words]

    return " ".join(filtered_words)

# Mengaplikasikan fungsi preprocess_text pada kolom 'text'
df['text'] = df['text'].apply(clean_text)

"""# Dataset Splitting"""

# Membagi data menjadi data train dan data test
X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Tokenisasi dan Padding"""

# Tokenisasi hanya pada data train
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# Mengubah teks menjadi sequence
train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

# Menentukan panjang maksimal berdasarkan data train
max_length = max([len(x) for x in train_sequences])

# Melakukan padding pada kedua set data
X_train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='pre')
X_test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='pre')

"""# Preprocessing Label"""

# Label Encoding dan One-hot Encoding
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

y_train_categorical = to_categorical(y_train_encoded)
y_test_categorical = to_categorical(y_test_encoded)

"""# Membangun Model Sequential dengan LSTM"""

# Membangun model
model = Sequential()
model.add(Embedding(len(tokenizer.word_index) + 1, 200, input_length=max_length, embeddings_initializer=glorot_normal()))
model.add(LSTM(100, return_sequences=True, dropout=0.6))
model.add(LSTM(100, dropout=0.6))
model.add(Dense(y_train_categorical.shape[1], activation='softmax'))

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# Training Model"""

# Callback untuk early stopping
callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Melatih model
history = model.fit(
    X_train_padded, y_train_categorical,
    epochs=10, batch_size=32,
    callbacks=[callback],
    validation_data=(X_test_padded, y_test_categorical))

"""# Evaluasi Model"""

# Evaluasi model pada data training
train_loss, train_accuracy = model.evaluate(X_train_padded, y_train_categorical)
print(f'Training Loss: {train_loss:.4f}')
print(f'Training Accuracy: {train_accuracy*100:.2f}%')

# Evaluasi model pada data test
test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_categorical)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy*100:.2f}%')

"""# Visualisasi Hasil Training"""

# Visualization of Training Loss
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Test')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Visualization of Training Accuracy
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Test')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

